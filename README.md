# SmartSound
The objective of this proposed system is to develop an intelligent audio control system real-time object detection algorithm for accurate people counting in a room. The system aims to dynamically adjust the volume of audio devices based on the real-time occupancy data and store the relevant information in a database. 

2.	Detailed Flow:

2.1.	Video Stream Processing:
In the video stream processing stage, the ESP32 camera serves as the source capturing the video feed within the room environment. Through its connection to an Arduino device, the camera generates a local URL, providing a gateway for accessing the live stream. Upon receiving this URL, the Python application establishes a connection to the video feed, leveraging the OpenCV library to efficiently read frames in real-time. OpenCV, a widely-used computer vision library, offers robust functionality for handling video input, facilitating frame extraction and processing seamlessly. By accessing the video stream programmatically, the Python application gains access to a continuous flow of visual data, enabling subsequent stages of analysis and processing. This approach ensures a streamlined and responsive system, as the Python application directly interfaces with the live video stream, bypassing the need for manual intervention or external storage. Overall, this stage forms the foundation for subsequent processing steps, providing the necessary input for human detection, volume control, and data logging within the system's architecture.


2.2. People Detection using YOLOv8 and Segment Anything (SAM):

STEP 1 :YOLOV8:
In the people detection stage utilizing YOLOv8, the Python application employs a state-of-the-art deep learning model specifically trained to detect various objects, including humans, within each frame of the video stream. YOLOv8, short for "You Only Look Once version 8," excels in real-time object detection tasks with remarkable accuracy and efficiency. As the video frames are fed into the YOLOv8 model, it analyzes the contents of each frame and identifies regions of interest corresponding to human subjects. These detections are represented as bounding boxes encompassing the detected individuals within the frame. Subsequently, the Python application extracts these bounding boxes from the model's output, enabling further processing and analysis. To enhance the accuracy of the detection results and address potential issues such as overlapping bounding boxes, a post-processing step may be implemented. This post-processing step could involve applying a segmentation model or algorithm designed to refine the detected regions, filter out redundant or overlapping detections, and improve the overall reliability of the detection process. By leveraging YOLOv8 for people detection and potentially incorporating additional post-processing techniques, the system ensures robust and accurate identification of human subjects within the video stream, laying the groundwork for subsequent actions such as volume adjustment based on the detected count of individuals.

2.3. Volume Control:


In the volume control stage, the system dynamically adjusts the audio volume based on the count of people detected within the room. This adaptive approach ensures that the volume level aligns with the current occupancy, optimizing the listening experience for individuals present. To facilitate volume adjustment, the system interfaces with the AudioEndpointVolume interface, which provides access to the system's audio devices and controls. Through this interface, the system can programmatically manipulate the audio volume in real-time, enabling seamless integration with the people detection process.
The volume adjustment strategy varies based on the number of people detected, with different volume levels set to accommodate different scenarios. For instance, in situations where no individuals are detected, the system may lower the volume or mute the audio output entirely to conserve energy and minimize disturbances. Conversely, as the count of detected individuals increases, the system gradually raises the volume to ensure adequate sound levels for comfortable listening. By dynamically adjusting the volume based on occupancy, the system enhances user experience and promotes efficient resource utilization.
Additionally, the use of the AudioEndpointVolume interface offers a versatile and platform-independent solution for volume control, allowing the system to adapt to various hardware configurations and operating environments seamlessly. This approach ensures compatibility across different devices and enables consistent performance across different deployments. Overall, the volume control stage plays a pivotal role in optimizing the auditory environment within the room, enhancing user comfort, and ensuring an immersive audio experience..

2.4. Data Logging:

In the data logging stage, the system logs live data such as the current time, day of the week, and volume set into a CSV file. Python's built-in csv module facilitates writing this data to the file in real-time as volume adjustments are made. This approach ensures that the system maintains an accurate record of volume control activities, allowing for monitoring, analysis, and future optimizations based on historical data. By logging essential parameters into a structured CSV format, the system enables easy access and interpretation of logged information for tracking system performance and making informed decisions.

2.5. Machine Learning Mode:

In the machine learning mode, historical data extracted from the CSV file serves as the foundation for training a decision tree regression model. This historical dataset contains crucial information such as the time, day of the week, and corresponding volume levels recorded over time. The Scikit-learn library, renowned for its comprehensive machine learning tools, is leveraged to implement and train the decision tree regression model effectively. During the training process, the features extracted from the historical dataset, namely time and day of the week, are utilized as input variables for the model. These features serve as predictors, influencing the model's predictions of the target attribute, which in this case is the volume level. By analyzing the relationships between these features and the target attribute, the decision tree regression model learns to make accurate volume level predictions based on the time and day of the week.
Once trained, the decision tree regression model becomes capable of predicting the volume level for a given input of the current time and day. This prediction capability empowers the system to dynamically adjust the volume level in real-time based on the prevailing conditions, ensuring optimal audio experience in response to changing environmental factors. By integrating machine learning into the volume control mechanism, the system gains adaptability and intelligence, enhancing its ability to cater to user preferences and optimize audio settings automatically.

2.6. User Interface:

In the current build, the Kivy framework creates a user-friendly interface with sliders, buttons, and labels for volume control, mode toggling, and status display. Users can adjust volume, pause detection, and switch between prediction and ML modes via buttons. In future iterations, the interface will transition to an Android app. Additionally, a button enables switching between prediction and ML modes, allowing users flexibility in choosing between real-time detection and data-driven volume control approaches. This seamless integration of interface controls enhances user experience and system adaptability.
As the system evolves in future builds, the interface will transition to an Android application, built using Android programming tools. This transition will enable the interface to seamlessly integrate with Android devices, offering a cohesive user experience across different platforms. By embracing Android programming, the system will leverage the extensive capabilities of the Android ecosystem, including native device features and seamless integration with 
